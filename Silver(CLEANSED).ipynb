{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd51439-f03a-42b5-8fb4-2aa5ea517677",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Connection to ADLS"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# \uD83D\uDD11 CONNECT AZURE DATABRICKS TO ADLS GEN2 (USING STORAGE KEY)\n",
    "# ============================================================\n",
    "\n",
    "# Replace with your real values\n",
    "account_name = \"stgfinancialdata\"        # storage account name\n",
    "container_name = \"cleansed\"                   # your ADLS container name\n",
    "account_key = \"KjIwv8urMrnqA+xB3jpcQ1mUKoI/lEdhHBIDxl4yrhZLTPvXWo9CMba+21uyhVExWD+4xeSSqWkM+AStOCwOZA==\"\n",
    " # from Azure Portal -> Access keys\n",
    "\n",
    "# Configure Spark for ADLS access\n",
    "spark.conf.set(f\"fs.azure.account.key.{account_name}.dfs.core.windows.net\", account_key)\n",
    "\n",
    "# Helper function for easy ABFSS path building\n",
    "def abfss(path):\n",
    "    return f\"abfss://{container_name}@{account_name}.dfs.core.windows.net{path}\"\n",
    "\n",
    "print(\"✅ Connected to ADLS Gen2 successfully!\")\n",
    "print(\"Example path:\", abfss(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "167f1a1e-ec0a-4c2d-b2ee-443160c538c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NOTEBOOK 2: BRONZE CSV → SILVER (Timestamp Based Partitioning)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "RAW_BASE_PATH     = f\"abfss://raw@{account_name}.dfs.core.windows.net\"\n",
    "SILVER_CSV_PATH   = f\"abfss://cleansed@{account_name}.dfs.core.windows.net\"\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. READ ALL RAW CSV FILES (all folders)\n",
    "# ----------------------------------------------------------\n",
    "bronze_df = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(f\"{RAW_BASE_PATH}/*/*.csv\")   # ANY monthly folder\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. CAST TIMESTAMP + BASIC TYPES\n",
    "# ----------------------------------------------------------\n",
    "df = (\n",
    "    bronze_df\n",
    "        .withColumn(\"Timestamp\", F.to_timestamp(\"Timestamp\"))\n",
    "        .withColumn(\"Account_Number\", F.col(\"Account_Number\").cast(\"long\"))\n",
    "        .withColumn(\"Opening_Balance\", F.col(\"Opening_Balance\").cast(\"double\"))\n",
    "        .withColumn(\"Amount\", F.col(\"Amount\").cast(\"double\"))\n",
    "        .withColumn(\"Closing_Balance\", F.col(\"Closing_Balance\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# display(df)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. CLEAN STRING COLUMNS\n",
    "# ----------------------------------------------------------\n",
    "string_cols = [c for c, t in df.dtypes if t == \"string\"]\n",
    "\n",
    "for c in string_cols:\n",
    "    df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "df = df.fillna({c: \"N/A\" for c in string_cols})\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. STANDARDISE CASE\n",
    "# ----------------------------------------------------------\n",
    "def initcap_safe(col): return F.when(F.col(col) != \"N/A\", F.initcap(F.col(col))).otherwise(\"N/A\")\n",
    "def upper_safe(col):   return F.when(F.col(col) != \"N/A\", F.upper(F.col(col))).otherwise(\"N/A\")\n",
    "\n",
    "df = df.withColumn(\"Merchant_Name\", initcap_safe(\"Merchant_Name\"))\n",
    "df = df.withColumn(\"Location\", initcap_safe(\"Location\"))\n",
    "df = df.withColumn(\"Country\", upper_safe(\"Country\"))\n",
    "df = df.withColumn(\"Status\", upper_safe(\"Status\"))\n",
    "df = df.withColumn(\"Channel\", upper_safe(\"Channel\"))\n",
    "df = df.withColumn(\"Transaction_Mode\", upper_safe(\"Transaction_Mode\"))\n",
    "df = df.withColumn(\"Currency\", upper_safe(\"Currency\"))\n",
    "df = df.withColumn(\"Type\", initcap_safe(\"Type\"))\n",
    "df = df.withColumn(\"Category\", upper_safe(\"Category\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5. SPLIT DATE & TIME\n",
    "# ----------------------------------------------------------\n",
    "df = df.withColumn(\"Date\", F.to_date(\"Timestamp\"))\n",
    "df = df.withColumn(\"Time\", F.date_format(\"Timestamp\", \"HH:mm:ss\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6. CATEGORY STANDARDISATION\n",
    "# ----------------------------------------------------------\n",
    "category_map = {\n",
    "    \"GROCERIES\": \"Groceries\",\n",
    "    \"FOOD\": \"Food\",\n",
    "    \"TRAVEL\": \"Travel\",\n",
    "    \"FUEL\": \"Fuel\",\n",
    "    \"PHARMACY\": \"Pharmacy\",\n",
    "    \"DIGITAL\": \"Digital\",\n",
    "    \"ENTERTAINMENT\": \"Entertainment\",\n",
    "    \"SHOPPING\": \"Shopping\",\n",
    "    \"ELECTRONICS\": \"Electronics\",\n",
    "    \"CLOTHING\": \"Clothing\",\n",
    "    \"SALARY\": \"Salary\",\n",
    "    \"BILLS\": \"Bills\",\n",
    "    \"CASH WITHDRAWAL\": \"Cash Withdrawal\",\n",
    "}\n",
    "\n",
    "for src, tgt in category_map.items():\n",
    "    df = df.withColumn(\n",
    "        \"Category\",\n",
    "        F.when(F.col(\"Category\") == src, tgt).otherwise(F.col(\"Category\"))\n",
    "    )\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Category\",\n",
    "    F.when(\n",
    "        (~F.col(\"Category\").isin(list(category_map.values()))) & (F.col(\"Category\") != \"N/A\"),\n",
    "        \"Other\"\n",
    "    ).otherwise(F.col(\"Category\"))\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 7. ROUND BALANCES TO 2 DECIMALS  <-- \uD83D\uDD25 NEW\n",
    "# ----------------------------------------------------------\n",
    "df = df.withColumn(\"Opening_Balance\", F.round(F.col(\"Opening_Balance\"), 2))\n",
    "df = df.withColumn(\"Closing_Balance\", F.round(F.col(\"Closing_Balance\"), 2))\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# 7. REMOVE DUPLICATES\n",
    "# ----------------------------------------------------------\n",
    "df = df.dropDuplicates([\"Transaction_ID\"])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 8. COMPUTE year_month from Timestamp\n",
    "# ----------------------------------------------------------\n",
    "df = df.withColumn(\"year_month\", F.date_format(\"Timestamp\", \"yyyy_MM\"))\n",
    "\n",
    "# display(df)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 9. GET UNIQUE MONTH LIST\n",
    "# ----------------------------------------------------------\n",
    "months = df.select(\"year_month\").distinct().collect()\n",
    "\n",
    "print(\"Detected months →\", months)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 10. WRITE SEPARATE SILVER TABLE FOR EACH MONTH\n",
    "# ----------------------------------------------------------\n",
    "for ym_row in months:\n",
    "    ym = ym_row[\"year_month\"]\n",
    "    year, month = ym.split(\"_\")\n",
    "    table_name = f\"silver_{year}_{month}\"\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCCC Writing table: {table_name}\")\n",
    "    \n",
    "    df_month = df.filter(F.col(\"year_month\") == ym)\n",
    "\n",
    "    # Write as Delta table\n",
    "    (\n",
    "        df_month.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(table_name)\n",
    "    )\n",
    "\n",
    "    # And also write CSV backup\n",
    "    (\n",
    "        df_month.coalesce(1).write\n",
    "            .option(\"header\", \"true\")\n",
    "            .mode(\"overwrite\")\n",
    "            .csv(f\"{SILVER_CSV_PATH}/{ym}\")\n",
    "    )\n",
    "\n",
    "    print(f\"✔ Table saved: {table_name}\")\n",
    "    print(f\"✔ CSV saved: {SILVER_CSV_PATH}/{ym}\")\n",
    "\n",
    "print(\"\\n\uD83C\uDF89 Silver processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver(CLEANSED)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}